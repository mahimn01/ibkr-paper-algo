"""
Fractional Differentiation for Memory-Preserving Stationarity.

Implements the Fixed-Width Window Fractional Differentiation (FFD) method from
Marcos Lopez de Prado, "Advances in Financial Machine Learning" (2018),
Chapter 5: Fractionally Differentiated Features.

The key insight: standard returns (d=1) achieve stationarity but destroy all
memory of price levels -- the autocorrelation structure that carries predictive
information is lost.  Raw prices (d=0) retain full memory but are
non-stationary, causing spurious regression in ML models.  Fractional
differentiation finds the minimum differentiation order *d* in (0, 1) that
achieves stationarity while preserving maximum memory.

Typical optimal d values fall in the range 0.3--0.7, producing features that
are simultaneously stationary (valid for regression / classification) AND
retain significant correlation with the original price series (memory).

The output of :func:`frac_diff_ffd` is intended to be used as input features
to the :class:`NonlinearSignalCombiner` (XGBoost) in
``scripts/train_signal_model.py``.

Weight formula (FFD):
    w_0 = 1
    w_k = -w_{k-1} * (d - k + 1) / k

The weights alternate in sign and decay; the series is truncated when
|w_k| < threshold, giving the fixed-width window.

References:
    - Lopez de Prado, M. (2018). *Advances in Financial Machine Learning*.
      John Wiley & Sons.  Chapter 5.
    - Hosking, J. R. M. (1981). "Fractional differencing". *Biometrika*,
      68(1), 165--176.
"""

from __future__ import annotations

import numpy as np
from numpy.typing import NDArray
from typing import Optional

from trading_algo.quant_core.utils.statistics import (
    augmented_dickey_fuller,
    AugmentedDickeyFullerResult,
)
from trading_algo.quant_core.utils.constants import EPSILON

__all__ = [
    "get_weights_ffd",
    "frac_diff_ffd",
    "find_optimal_d",
    "frac_diff_multi",
    "memory_stationarity_report",
]


# =============================================================================
# 1. FFD WEIGHTS
# =============================================================================

def get_weights_ffd(d: float, threshold: float = 1e-5) -> NDArray[np.float64]:
    """
    Compute Fixed-Width Window Fractional Differentiation weights.

    The weight sequence is generated by the recurrence:

        w_0 = 1
        w_k = -w_{k-1} * (d - k + 1) / k

    Weights are truncated once |w_k| drops below *threshold*, yielding the
    fixed-width window that defines the kernel length.  The weights alternate
    in sign and decay geometrically, implementing fractional differencing of
    order *d*.

    Args:
        d: Fractional differentiation order.  Must be >= 0.
            d=0 gives a single weight [1] (identity).
            d=1 gives weights [1, -1] (first difference).
            d in (0, 1) gives the fractional interpolation.
        threshold: Minimum absolute weight to retain.  Smaller values
            produce longer kernels (more memory) but slower computation.

    Returns:
        1-D array of FFD weights, shape ``(width,)``.  The first element
        corresponds to the most recent observation (w_0 = 1).

    References:
        Lopez de Prado (2018), Section 5.4, Snippet 5.3.

    Examples:
        >>> weights = get_weights_ffd(0.5, threshold=1e-4)
        >>> weights[0]
        1.0
        >>> len(weights) > 1
        True
    """
    if d < 0:
        raise ValueError(f"Differentiation order d must be >= 0, got {d}")

    weights = [1.0]
    k = 1
    while True:
        w_k = -weights[-1] * (d - k + 1) / k
        if abs(w_k) < threshold:
            break
        weights.append(w_k)
        k += 1

    return np.array(weights, dtype=np.float64)


# =============================================================================
# 2. FRACTIONAL DIFFERENTIATION (FFD METHOD)
# =============================================================================

def frac_diff_ffd(
    series: NDArray[np.float64],
    d: float,
    threshold: float = 1e-5,
) -> NDArray[np.float64]:
    """
    Apply Fixed-Width Window Fractional Differentiation to a price series.

    For each position ``i >= width - 1`` the output is the dot product of the
    reversed weight vector and the corresponding window of the input series::

        output[i] = sum(weights[::-1] * series[i - width + 1 : i + 1])

    Positions ``0`` through ``width - 2`` are filled with NaN because
    insufficient history is available to compute the full convolution.

    Args:
        series: 1-D price series (e.g. log prices or raw prices).
        d: Fractional differentiation order (0 <= d <= 2, typically 0--1).
        threshold: Weight truncation threshold passed to
            :func:`get_weights_ffd`.

    Returns:
        Fractionally differentiated series, same length as *series*.
        The first ``width - 1`` elements are NaN.

    Raises:
        ValueError: If the series is empty or d < 0.

    References:
        Lopez de Prado (2018), Section 5.4, Snippet 5.4.

    Examples:
        >>> prices = np.array([100.0, 101.0, 102.0, 101.5, 103.0])
        >>> fd = frac_diff_ffd(prices, d=0.5)
        >>> np.isnan(fd[0])  # first elements are NaN due to kernel width
        True
    """
    series = np.asarray(series, dtype=np.float64)
    n = len(series)

    if n == 0:
        return np.array([], dtype=np.float64)

    if d < 0:
        raise ValueError(f"Differentiation order d must be >= 0, got {d}")

    # d=0 is the identity transform
    if d == 0.0:
        return series.copy()

    weights = get_weights_ffd(d, threshold)
    width = len(weights)

    # Adaptively increase threshold until kernel fits the data.
    # For small d the weights decay very slowly, producing kernels with
    # thousands of elements.  Rather than returning all-NaN, progressively
    # relax the threshold so at least half the series is usable.
    max_width = max(n // 2, 20)
    adaptive_threshold = threshold
    while width > max_width and adaptive_threshold < 0.1:
        adaptive_threshold *= 10
        weights = get_weights_ffd(d, adaptive_threshold)
        width = len(weights)

    if width > n:
        return np.full(n, np.nan, dtype=np.float64)

    # Pre-reverse weights for the dot product: weights_rev[j] corresponds to
    # series[i - width + 1 + j].
    weights_rev = weights[::-1]

    output = np.full(n, np.nan, dtype=np.float64)

    # Handle NaN values in the input by checking each window
    for i in range(width - 1, n):
        window = series[i - width + 1: i + 1]
        if np.any(np.isnan(window)):
            output[i] = np.nan
        else:
            output[i] = np.dot(weights_rev, window)

    return output


# =============================================================================
# 3. FIND OPTIMAL d
# =============================================================================

def find_optimal_d(
    series: NDArray[np.float64],
    max_d: float = 1.0,
    step: float = 0.05,
    p_threshold: float = 0.05,
    threshold: float = 1e-5,
) -> tuple[float, dict]:
    """
    Find the minimum fractional differentiation order that achieves stationarity.

    Iterates over ``d = 0, step, 2*step, ..., max_d``, applying
    :func:`frac_diff_ffd` at each level and testing for stationarity with
    the Augmented Dickey-Fuller (ADF) test.  Returns the smallest *d* whose
    ADF p-value falls below *p_threshold*.

    Args:
        series: 1-D price series.
        max_d: Maximum differentiation order to test (inclusive).
        step: Increment between tested d values.
        p_threshold: ADF p-value threshold for declaring stationarity.
        threshold: Weight truncation threshold for FFD.

    Returns:
        A tuple ``(optimal_d, info)`` where:

        - ``optimal_d`` (float): The minimum d achieving stationarity.
          Returns ``max_d`` if no tested d is stationary.
        - ``info`` (dict): Diagnostic information with keys:

          - ``"optimal_d"`` -- same as the first return value.
          - ``"adf_stats"`` -- dict mapping d (float) to
            ``(adf_statistic, p_value)``.
          - ``"correlation_with_original"`` -- dict mapping d (float) to
            the Pearson correlation between the fractionally differentiated
            series and the original series (memory retention measure).

    References:
        Lopez de Prado (2018), Section 5.5.

    Examples:
        >>> np.random.seed(42)
        >>> prices = np.cumsum(np.random.randn(500)) + 100
        >>> opt_d, info = find_optimal_d(prices)
        >>> 0.0 <= opt_d <= 1.0
        True
    """
    series = np.asarray(series, dtype=np.float64)

    # Remove leading/trailing NaN
    valid_mask = ~np.isnan(series)
    if not np.any(valid_mask):
        return max_d, {
            "optimal_d": max_d,
            "adf_stats": {},
            "correlation_with_original": {},
        }

    # Trim to valid range
    first_valid = np.argmax(valid_mask)
    last_valid = len(series) - 1 - np.argmax(valid_mask[::-1])
    clean_series = series[first_valid: last_valid + 1]

    if len(clean_series) < 20:
        return max_d, {
            "optimal_d": max_d,
            "adf_stats": {},
            "correlation_with_original": {},
        }

    # Check if series is constant
    if np.std(clean_series) < EPSILON:
        return 0.0, {
            "optimal_d": 0.0,
            "adf_stats": {0.0: (float("-inf"), 0.0)},
            "correlation_with_original": {0.0: 1.0},
        }

    d_values = np.arange(0, max_d + step / 2, step)
    adf_stats: dict[float, tuple[float, float]] = {}
    correlations: dict[float, float] = {}
    optimal_d = max_d

    for d in d_values:
        d = round(float(d), 10)  # Avoid floating-point artifacts

        fd_series = frac_diff_ffd(clean_series, d, threshold)

        # Drop NaN values
        valid = fd_series[~np.isnan(fd_series)]

        if len(valid) < 20:
            adf_stats[d] = (0.0, 1.0)
            correlations[d] = np.nan
            continue

        # ADF test
        adf_result: AugmentedDickeyFullerResult = augmented_dickey_fuller(valid)
        adf_stats[d] = (adf_result.adf_statistic, adf_result.p_value)

        # Correlation with original (memory retention)
        # Align lengths: use the tail of the original series matching
        # the non-NaN portion of the fractionally differentiated series
        n_valid = len(valid)
        original_tail = clean_series[-n_valid:]

        # Handle constant fractionally-differentiated series
        if np.std(valid) < EPSILON or np.std(original_tail) < EPSILON:
            correlations[d] = 0.0 if d > 0 else 1.0
        else:
            correlations[d] = float(np.corrcoef(original_tail, valid)[0, 1])

        # Check stationarity
        if adf_result.p_value < p_threshold:
            optimal_d = d
            break

    info = {
        "optimal_d": optimal_d,
        "adf_stats": adf_stats,
        "correlation_with_original": correlations,
    }

    return optimal_d, info


# =============================================================================
# 4. BATCH FRACTIONAL DIFFERENTIATION
# =============================================================================

def frac_diff_multi(
    data: dict[str, NDArray[np.float64]],
    d: float | dict[str, float] | None = None,
    auto_d: bool = True,
) -> dict[str, NDArray[np.float64]]:
    """
    Apply fractional differentiation to multiple series (multi-symbol).

    Supports three modes:

    1. **auto_d=True, d=None** (default): Find the optimal d for each symbol
       independently using :func:`find_optimal_d`, then apply it.
    2. **d is float**: Use the same d for all symbols.
    3. **d is dict**: Use per-symbol d values (keys must match *data* keys).

    Args:
        data: Dictionary mapping symbol names to 1-D price series.
        d: Differentiation order.  Can be a single float (applied to all),
           a dict mapping symbol to float, or None (requires auto_d=True).
        auto_d: If True and *d* is None, automatically find optimal d for
           each symbol.

    Returns:
        Dictionary mapping symbol to fractionally differentiated series.

    Raises:
        ValueError: If d is None and auto_d is False.

    Examples:
        >>> data = {"SPY": np.cumsum(np.random.randn(300)) + 100,
        ...         "TLT": np.cumsum(np.random.randn(300)) + 50}
        >>> result = frac_diff_multi(data, d=0.5)
        >>> set(result.keys()) == {"SPY", "TLT"}
        True
    """
    if d is None and not auto_d:
        raise ValueError(
            "Either provide d (float or dict) or set auto_d=True."
        )

    result: dict[str, NDArray[np.float64]] = {}

    for symbol, series in data.items():
        series = np.asarray(series, dtype=np.float64)

        # Determine d for this symbol
        if isinstance(d, dict):
            symbol_d = d.get(symbol)
            if symbol_d is None:
                raise ValueError(
                    f"d dict does not contain key for symbol '{symbol}'."
                )
        elif isinstance(d, (int, float)):
            symbol_d = float(d)
        elif d is None and auto_d:
            symbol_d, _ = find_optimal_d(series)
        else:
            raise ValueError(
                f"Unexpected d type: {type(d)}. Expected float, dict, or None."
            )

        result[symbol] = frac_diff_ffd(series, symbol_d)

    return result


# =============================================================================
# 5. MEMORY-STATIONARITY TRADEOFF REPORT
# =============================================================================

def memory_stationarity_report(
    series: NDArray[np.float64],
    name: str = "series",
) -> str:
    """
    Generate a formatted report of the memory-stationarity tradeoff.

    For each ``d`` in ``{0.0, 0.1, 0.2, ..., 1.0}``, the report shows:

    - ADF test statistic and p-value (stationarity evidence)
    - Pearson correlation with the original series (memory retention)
    - Number of valid (non-NaN) observations after differentiation

    Additionally reports the optimal d (minimum for stationarity at 5%) and
    the memory retention at that level.

    This output is designed for the ``pattern_discovery.py`` analysis pipeline.

    Args:
        series: 1-D price series to analyze.
        name: Label for the series in the report header.

    Returns:
        Multi-line formatted string suitable for printing or logging.

    Examples:
        >>> prices = np.cumsum(np.random.randn(500)) + 100
        >>> report = memory_stationarity_report(prices, name="SPY")
        >>> "Optimal d" in report
        True
    """
    series = np.asarray(series, dtype=np.float64)
    valid_mask = ~np.isnan(series)

    if not np.any(valid_mask) or len(series) < 20:
        return (
            f"Memory-Stationarity Report: {name}\n"
            f"{'=' * 70}\n"
            f"Insufficient data (n={len(series)}). Minimum 20 observations required.\n"
        )

    # Trim to valid range
    first_valid = int(np.argmax(valid_mask))
    last_valid = len(series) - 1 - int(np.argmax(valid_mask[::-1]))
    clean_series = series[first_valid: last_valid + 1]

    lines: list[str] = []
    lines.append(f"Memory-Stationarity Report: {name}")
    lines.append("=" * 70)
    lines.append(f"Series length: {len(clean_series)} observations")
    lines.append(f"Price range: [{clean_series.min():.4f}, {clean_series.max():.4f}]")
    lines.append("")
    lines.append(
        f"{'d':>5s}  {'ADF Stat':>10s}  {'p-value':>10s}  "
        f"{'Corr w/ Orig':>13s}  {'Valid Obs':>10s}  {'Stationary':>11s}"
    )
    lines.append("-" * 70)

    d_values = np.arange(0.0, 1.05, 0.1)
    optimal_d: Optional[float] = None
    optimal_corr: Optional[float] = None

    for d in d_values:
        d = round(float(d), 1)

        fd = frac_diff_ffd(clean_series, d)
        valid = fd[~np.isnan(fd)]
        n_valid = len(valid)

        if n_valid < 20:
            lines.append(
                f"{d:5.1f}  {'---':>10s}  {'---':>10s}  "
                f"{'---':>13s}  {n_valid:>10d}  {'---':>11s}"
            )
            continue

        # ADF test
        adf_result = augmented_dickey_fuller(valid)
        adf_stat = adf_result.adf_statistic
        p_value = adf_result.p_value
        is_stationary = p_value < 0.05

        # Correlation with original
        original_tail = clean_series[-n_valid:]
        if np.std(valid) < EPSILON or np.std(original_tail) < EPSILON:
            corr = 0.0 if d > 0 else 1.0
        else:
            corr = float(np.corrcoef(original_tail, valid)[0, 1])

        stationary_str = "YES" if is_stationary else "no"

        lines.append(
            f"{d:5.1f}  {adf_stat:10.4f}  {p_value:10.6f}  "
            f"{corr:13.4f}  {n_valid:>10d}  {stationary_str:>11s}"
        )

        # Track optimal d
        if is_stationary and optimal_d is None:
            optimal_d = d
            optimal_corr = corr

    lines.append("-" * 70)

    if optimal_d is not None:
        lines.append(f"Optimal d: {optimal_d:.1f}")
        lines.append(f"Memory retention at optimal d: {optimal_corr:.4f}")
        lines.append(
            f"Interpretation: {_interpret_d(optimal_d, optimal_corr)}"
        )
    else:
        lines.append(
            "No d in [0.0, 1.0] achieved stationarity at 5% significance."
        )
        lines.append("Consider using d=1.0 (standard returns) or a longer series.")

    lines.append("")
    lines.append("Reference: Lopez de Prado (2018), Chapter 5")
    lines.append(
        "Note: Lower d = more memory retained, higher d = more stationary."
    )

    return "\n".join(lines)


def _interpret_d(d: float, corr: float) -> str:
    """Generate a human-readable interpretation of the optimal d and correlation."""
    if d <= 0.3:
        d_desc = "Very low differentiation -- series close to raw prices."
    elif d <= 0.5:
        d_desc = "Moderate differentiation -- good balance of memory and stationarity."
    elif d <= 0.7:
        d_desc = "Moderate-high differentiation -- some memory loss acceptable."
    else:
        d_desc = "High differentiation -- approaching standard returns."

    if corr >= 0.9:
        corr_desc = "Excellent memory retention."
    elif corr >= 0.7:
        corr_desc = "Good memory retention."
    elif corr >= 0.5:
        corr_desc = "Moderate memory retention."
    else:
        corr_desc = "Significant memory loss."

    return f"{d_desc} {corr_desc}"
